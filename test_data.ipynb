{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.models as models\n",
    "from transform_cnn import VA\n",
    "from data_cnn import NTUDataLoaders, AverageMeter,  make_dir, get_cases, get_num_classes\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.ArgumentParser(description='View adaptive')\n",
    "args.add_argument('--model', type=str, default='VA',\n",
    "                  help='the neural network to use')\n",
    "args.add_argument('--dataset', type=str, default='NTU',\n",
    "                  help='select dataset to evlulate')\n",
    "args.add_argument('--max_epoches', type=int, default=100,\n",
    "                  help='start number of epochs to run')\n",
    "args.add_argument('--lr', type=float, default=0.0001,\n",
    "                  help='initial learning rate')\n",
    "args.add_argument('--lr_factor', type=float, default=0.1,\n",
    "                  help='the ratio to reduce lr on each step')\n",
    "args.add_argument('--optimizer', type=str, default='Adam',\n",
    "                  help='the optimizer type')\n",
    "args.add_argument('--print_freq', '-p', type=int, default=20,\n",
    "                  help='print frequency (default: 20)')\n",
    "args.add_argument('-b', '--batch_size', type=int, default=1,\n",
    "                  help='mini-batch size (default: 256)')\n",
    "args.add_argument('--num_classes', type=int, default=60,\n",
    "                  help='the number of classes')\n",
    "args.add_argument('--case', type=int, default=1,\n",
    "                  help='select which case')\n",
    "args.add_argument('--aug', type=int, default=1,\n",
    "                  help='data augmentation')\n",
    "args.add_argument('--workers', type=int, default=8,\n",
    "                  help='number of data loading workers')\n",
    "args.add_argument('--monitor', type=str, default='val_acc',\n",
    "                  help='quantity to monitor (default: val_acc)')\n",
    "args.add_argument('--train', type=int, default=1,\n",
    "                  help='train or test')\n",
    "args = args.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = get_num_classes(args.dataset)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35763 samples, validate on 20815 samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = get_num_classes(args.dataset)\n",
    "# if args.model[0:2] == 'VA':\n",
    "#     model = VA(num_classes)\n",
    "# else:\n",
    "#     model = models.resnet50(pretrained=True)\n",
    "#     num_ftrs = model.fc.in_features\n",
    "#     model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "# # define loss function (criterion) and optimizer\n",
    "# criterion = nn.CrossEntropyLoss().cuda()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# if args.monitor == 'val_acc':\n",
    "#     mode = 'max'\n",
    "#     monitor_op = np.greater\n",
    "#     best = -np.Inf\n",
    "#     str_op = 'improve'\n",
    "# elif args.monitor == 'val_loss':\n",
    "#     mode = 'min'\n",
    "#     monitor_op = np.less\n",
    "#     best = np.Inf\n",
    "#     str_op = 'reduce'\n",
    "# if args.dataset=='NTU' or args.dataset == 'PKU':\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode=mode, factor=args.lr_factor,\n",
    "#                               patience=2, cooldown=2, verbose=True)\n",
    "# else:\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode=mode, factor=args.lr_factor,\n",
    "#                                   patience=5, cooldown=3, verbose=True)\n",
    "\n",
    "# Data loading\n",
    "ntu_loaders = NTUDataLoaders(args.dataset, args.case, args.aug)\n",
    "train_loader = ntu_loaders.get_train_loader(args.batch_size, args.workers)\n",
    "val_loader = ntu_loaders.get_val_loader(args.batch_size, args.workers)\n",
    "train_size = ntu_loaders.get_train_size()\n",
    "val_size = ntu_loaders.get_val_size()\n",
    "print('Train on %d samples, validate on %d samples' %\n",
    "      (train_size, val_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "#action=[40,41,42,43,44,45,46,47,48]\n",
    "\n",
    "for i, (inputs, maxmin, target) in enumerate(train_loader):\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    target = target.to(device) \n",
    "\n",
    "    act= int(target)\n",
    "    #if act in action:\n",
    "    filename_format = \"image-{0}-{1}.png\"\n",
    "    #print(act)\n",
    "\n",
    "    base_dir = \"/home/fatema/Documents/action_rec/project/img_ske_all/train\"\n",
    "\n",
    "\n",
    "    image_path_dir = os.path.join(base_dir, str(act))\n",
    "    if not os.path.exists(image_path_dir):\n",
    "        os.mkdir(image_path_dir)\n",
    "    file_count = str(len(os.listdir(image_path_dir))+1)\n",
    "    image_save_path = os.path.join(image_path_dir, filename_format.format(str(act), file_count))\n",
    "\n",
    "    #print(inputs.shape)\n",
    "    #print(type(inputs)) \n",
    "    #print(labels)\n",
    "    data=torch.squeeze(inputs, 0)\n",
    "    #print(data.shape)\n",
    "    data=data[0]\n",
    "    #print(data.shape)\n",
    "    data=data.cpu().numpy()    \n",
    "\n",
    "    #print(data.shape)\n",
    "\n",
    "    matplotlib.image.imsave(image_save_path, data)\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "#action=list(range(59))\n",
    "for i, (inputs, maxmin, target) in enumerate(val_loader):\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    target = target.to(device) \n",
    "\n",
    "    act= int(target)\n",
    "    #if act in action:\n",
    "    filename_format = \"image-{0}-{1}.png\"\n",
    "    #print(act)\n",
    "\n",
    "    base_dir = \"/home/fatema/Documents/action_rec/project/img_ske_all/val\"\n",
    "\n",
    "\n",
    "    image_path_dir = os.path.join(base_dir, str(act))\n",
    "    if not os.path.exists(image_path_dir):\n",
    "        os.mkdir(image_path_dir)\n",
    "    file_count = str(len(os.listdir(image_path_dir))+1)\n",
    "    image_save_path = os.path.join(image_path_dir, filename_format.format(str(act), file_count))\n",
    "\n",
    "    #print(inputs.shape)\n",
    "    #print(type(inputs)) \n",
    "    #print(labels)\n",
    "    data=torch.squeeze(inputs, 0)\n",
    "    #print(data.shape)\n",
    "    data=data[0]\n",
    "    #print(data.shape)\n",
    "    data=data.cpu().numpy()    \n",
    "\n",
    "    #print(data.shape)\n",
    "\n",
    "    matplotlib.image.imsave(image_save_path, data)\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5397\n",
      "3125\n"
     ]
    }
   ],
   "source": [
    "val_y=[]\n",
    "train_y=[]\n",
    "\n",
    "action=[40,41,42,43,44,45,46,47,48]\n",
    "for i, (inputs, maxmin, target) in enumerate(train_loader):\n",
    "        c=int(target)\n",
    "        if c in action:\n",
    "            train_y.append(c)\n",
    "\n",
    "print(len(train_y))\n",
    "           \n",
    "for i, (inputs, maxmin, target) in enumerate(val_loader):\n",
    "        c=int(target)\n",
    "        if c in action:\n",
    "            val_y.append(c)\n",
    "\n",
    "print(len(val_y))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=torch.squeeze(inputs, 0)\n",
    "# print(data.shape)\n",
    "# data=data[0]\n",
    "# print(data.shape)\n",
    "# data=data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.imshow(x.astype('uint8'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
